{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets\n",
    "In this notebook, we link together customer, merchant, transaction, and customer demographic data. This is performed in four key stages:\n",
    "\n",
    "1. [Join BNPL data (consumers, merchants, transactions)](#join-customermerchanttransaction-data)\n",
    "2. [Prepare census data for merging](#prepare-external-datasets)\n",
    "3. [Join BNPL data with census data](#join-abs-and-customer-data)\n",
    "\n",
    "# Join customer/merchant/transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data Joining\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"9g\") \n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.network.timeout\", \"600s\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in datasets\n",
    "# Load in merchant data (parquet)\n",
    "merchant = spark.read.parquet(\"../data/curated/part_1/clean_merchant.parquet\")\n",
    "\n",
    "# Load in merchant fraud (csv)\n",
    "merchant_fp = pd.read_csv(\"../data/tables/part_1/merchant_fraud_probability.csv\")\n",
    "merchant_fp = spark.createDataFrame(merchant_fp)\n",
    "\n",
    "# Load in consumer list (csv)\n",
    "consumer_cid = pd.read_csv(\"../data/tables/part_1/tbl_consumer.csv\", delimiter=\"|\")\n",
    "consumer_cid = spark.createDataFrame(consumer_cid)\n",
    "\n",
    "# Load in consumer fraud (csv)\n",
    "consumer_fp = pd.read_csv(\"../data/tables/part_1/consumer_fraud_probability.csv\")\n",
    "consumer_fp = spark.createDataFrame(consumer_fp)\n",
    "\n",
    "consumer_ud = spark.read.parquet(\"../data/tables/part_1/consumer_user_details.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare customer data**: associate customers with a `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining user id to customers\n",
    "consumer_cid = consumer_cid.withColumn(\"postcode\", consumer_cid.postcode.cast('string')) # cast postcode to string\n",
    "consumer = consumer_cid.join(consumer_ud, on = \"consumer_id\", how = 'left')\n",
    "consumer_list = consumer.select(\"user_id\", \"postcode\")\n",
    "consumer.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join customers and transaction data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transaction dataset\n",
    "transaction1 = spark.read.parquet(\"../data/tables/part_2\")\n",
    "transaction2 = spark.read.parquet(\"../data/tables/part_3\")\n",
    "transaction3 = spark.read.parquet(\"../data/tables/part_4\")\n",
    "\n",
    "transaction = transaction1.union(transaction2).union(transaction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join customers to transactions\n",
    "transaction_consumer = transaction.join(consumer_list, on='user_id', how='left')\n",
    "transaction_consumer.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_no_transaction = consumer_list.join(transaction, on='user_id', how='left_anti')\n",
    "print(f\"Number of consumers that have not made a transaction: {consumer_no_transaction.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining customer transaction to merchant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for any multiple entries in consumer_fp\n",
    "consumer_fp = consumer_fp.groupBy('order_datetime', 'user_id').agg(F.max('fraud_probability').alias('fraud_probability'))\n",
    "\n",
    "# Add consumer fraud to transactions\n",
    "final_df = transaction_consumer.join(consumer_fp, on=['order_datetime', 'user_id'], how='left').withColumnRenamed('fraud_probability', 'consumer_fraud')\n",
    "no_fraud = final_df.filter(F.col(\"consumer_fraud\").isNull()).count()\n",
    "print(f\"Number of transactions with no consumer fraud: {no_fraud:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for any multiple entries in merchant_fp\n",
    "merchant_fp = merchant_fp.groupBy('order_datetime', 'merchant_abn').agg(F.max('fraud_probability').alias('fraud_probability'))\n",
    "\n",
    "# Add merchant fraud to transactions by merchant and date\n",
    "final_df = final_df.join(merchant_fp, on=['merchant_abn','order_datetime'], how = 'left').withColumnRenamed('fraud_probability', 'merchant_fraud')\n",
    "no_fraud = final_df.filter(F.col(\"merchant_fraud\").isNull()).count()\n",
    "print(f\"Number of transactions with no merchant fraud: {no_fraud:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute all null fraud probabilities as 0\n",
    "final_df = final_df.fillna(0, subset=['merchant_fraud', 'consumer_fraud'])\n",
    "no_fraud = final_df.filter((final_df[\"consumer_fraud\"]==0) & (final_df[\"merchant_fraud\"]==0)).count()\n",
    "print(f\"Number of transactions with no merchant fraud or consumer fraud: {no_fraud:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save transaction-merchant-consumer data to file\n",
    "final_df.write.mode('overwrite').parquet('../data/curated/fraud_watch/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare external datasets\n",
    "Here, we estimate weekly disposible income based on the difference between total_personal_income and the average spent on rent or morgage repayments per week. The calculation uses weekly variables as follows.\n",
    "$$\\text{weekly disposible income} = \\text{total personal income} - (\\text{median rent} \\times \\text{proportion of renters}) - (\\text{median morgage repayment} \\times \\text{proportion of mortgage holders})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download housing data\n",
    "file_path = '../data/tables/sa2_dataset/main/C21_G37_SA2.csv'\n",
    "url = \"https://api.data.abs.gov.au/data/C21_G37_SA2/1+2+R_T+_T...SA2..2021.?detail=full\"\n",
    "headers = {'accept': 'text/csv'}\n",
    "\n",
    "# Request data\n",
    "response = requests.get(url, headers=headers, stream=True)\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        if chunk:\n",
    "            file.write(chunk)\n",
    "\n",
    "# Read in data and cast types\n",
    "data = pd.read_csv(file_path, dtype={\"REGION\": object})\n",
    "\n",
    "variables = {\"R_T\":'renting',\n",
    "             \"2\":'owned_mortgage',\n",
    "             \"1\": 'owned_outright',\n",
    "             \"_T\":'total_responses',\n",
    "             \"REGION\": 'sa2_code'}\n",
    "\n",
    "# Aggregate to ignore the 'dwelling type' feature\n",
    "tenure_data = data.groupby(['REGION', 'TENLLD']).agg('sum').reset_index()[['REGION', 'TENLLD', 'OBS_VALUE']]\n",
    "tenure_data = tenure_data.pivot(index='REGION', columns='TENLLD', values='OBS_VALUE').reset_index()\n",
    "tenure_data = tenure_data.rename(variables, axis=1)\n",
    "tenure_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply calculations\n",
    "tenure_data['percent_mortgage'] = tenure_data['owned_mortgage'] / tenure_data['total_responses']\n",
    "tenure_data['percent_rent'] = tenure_data['renting'] / tenure_data['total_responses']\n",
    "\n",
    "# Investigate number of records with missing data\n",
    "zero_responses = tenure_data[tenure_data.isna().any(axis=1)]\n",
    "print('Number of SA2 zones with no data from ABS: ', len(zero_responses))\n",
    "zero_responses.head(5)\n",
    "\n",
    "# Handle missing null values by setting to zero\n",
    "percentage_tenure = tenure_data.fillna(0, axis=1).iloc[:,[0, -1, -2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset with median statistics\n",
    "variables = {1: \"median_age\", \n",
    "             2: \"median_total_personal_income\",\n",
    "             3: \"median_total_family_income\",\n",
    "             4: \"median_total_household_income\",\n",
    "             5: \"median_mortgage_repayment\",\n",
    "             6: \"median_rent\",\n",
    "             7: \"avg_people_per_bedroom\",\n",
    "             8: \"avg_household_size\"}\n",
    "\n",
    "# Read in data\n",
    "medians = pd.read_csv(\"../data/curated/sa2_dataset/C21_G02_SA2_clean.csv\")\n",
    "\n",
    "# Restructure table\n",
    "medians = medians.pivot(index='sa2_code', columns=['type_of_value_code'], values='obs_value').reset_index().rename(columns=variables)\n",
    "medians.columns.name = None\n",
    "medians['sa2_code'] = medians.sa2_code.astype(str)\n",
    "medians.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before joining housing data to this table of medians, we investigate the records and notice that there are records that have null median summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in list of SA2 codes and associated names\n",
    "col_types = {\"POSTCODE\": str, \"SA2_CODE_2021\":str, \"RATIO_FROM_TO\": float}\n",
    "sa2_names = pd.read_excel(\"../data/tables/correspondence/CG_POSTCODE_2021_SA2_2021.xlsx\", converters=col_types)[['SA2_CODE_2021', 'SA2_NAME_2021', 'POSTCODE']]\n",
    "\n",
    "# Find records with null columns\n",
    "null_regions = medians[medians.isna().any(axis=1)]\n",
    "null_regions = null_regions.merge(sa2_names, left_on='sa2_code', right_on='SA2_CODE_2021')\n",
    "\n",
    "# Show the name of regions associated with null values \n",
    "null_regions.iloc[:,-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the external dataset is only missing values for SA2 regions marked as having \"no usual address\", and this is ok since they are special purpose codes that don't correspond with a SA2 zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEKS_IN_MONTH = 4.345\n",
    "\n",
    "# Add engineered housing features to ABS demographic data\n",
    "abs_df = medians.merge(percentage_tenure, on='sa2_code')\n",
    "\n",
    "# Calculate average weekly spending on housing per SA2 zone\n",
    "# note: monthly mortgage repayment converted to weekly by dividing by # weeks in a month\n",
    "abs_df['avg_housing_weekly'] = abs_df.median_rent*abs_df.percent_rent + abs_df.median_mortgage_repayment*(abs_df.percent_mortgage/WEEKS_IN_MONTH)\n",
    "abs_df['weekly_personal_disposable'] = abs_df.median_total_personal_income - abs_df.avg_housing_weekly\n",
    "\n",
    "# Associate postcodes to SA2 data using \"correspondence.parquet\"\n",
    "sa2_names = pd.read_parquet('../data/curated/correspondence.parquet')\n",
    "abs_df = sa2_names.merge(abs_df, on='sa2_code', how='left')\n",
    "\n",
    "# Remove unecessary columns\n",
    "abs_df = abs_df.drop(['median_mortgage_repayment', 'median_rent', 'percent_rent', \n",
    "                      'percent_mortgage', 'avg_housing_weekly', \n",
    "                      'median_total_personal_income', 'avg_people_per_bedroom'], axis=1)\n",
    "abs_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "abs_df.to_parquet('../data/curated/sa2_dataset/abs_medians.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join ABS and customer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in abs demographic data and customer/transaction data\n",
    "abs_df = spark.read.parquet(\"../data/curated/sa2_dataset/abs_medians.parquet\")\n",
    "\n",
    "try:\n",
    "    final_df = spark.read.parquet(\"../data/curated/fraud_watch\")\n",
    "except:\n",
    "    final_df = final_df\n",
    "\n",
    "# Join ABS data to transactions records\n",
    "customer_details_abs = final_df.join(abs_df, on='postcode', how='left')\n",
    "\n",
    "# Check NULL values for transaction data\n",
    "customer_details_abs.summary('count').toPandas().to_dict(orient='records')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are some customer records that are to locations that have no population data. Many of these postcodes are for PO boxes and LVR addresses. For this, we create another field `is_po_box` to help filter out transactions to users delivering to special postcodes.\n",
    "\n",
    "The ranges for LVRs and PO Boxes were sourced from [Matthew Procter's Australian Postcodes website](https://www.matthewproctor.com/australian_postcodes#downloadlinks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deine PO/LVR range\n",
    "po_box_codes = [(\"1000\", \"1999\"), (\"0200\", \"0299\"), (\"8000\", \"8999\"),\n",
    "                (\"9000\", \"9999\"), (\"5800\", \"5999\"), (\"6800\", \"6999\"),\n",
    "                (\"7800\", \"7999\"), (\"0900\", \"0999\")]\n",
    "\n",
    "# Determine if postcode is in a PO box/LVR range\n",
    "po_box_condition = F.lit(False)\n",
    "for lower, upper in po_box_codes:\n",
    "    po_box_condition = po_box_condition | ((F.col(\"postcode\") >= lower) & (F.col(\"postcode\") <= upper))\n",
    "\n",
    "# Apply condition\n",
    "customer_details_abs = customer_details_abs.withColumn('is_po_box', po_box_condition)\n",
    "\n",
    "# Confirm that transactions with no associated ABS data delivers to a PO box\n",
    "customer_details_abs.filter(\"sa2_code IS NOT NULL AND is_po_box IS NULL\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify column order to see features better\n",
    "col_order = [\"order_id\", \"user_id\",\"merchant_abn\", \"order_datetime\", \"dollar_value\", \n",
    "              \"postcode\", \"merchant_fraud\", \"consumer_fraud\",\"weekly_personal_disposable\",\n",
    "              \"median_total_household_income\", \"median_total_family_income\"]\n",
    "\n",
    "# Save changes to \"all_details\"\n",
    "customer_details_abs.select(*col_order, *(set(customer_details_abs.columns) ^ set(col_order)))\\\n",
    "    .write.mode('overwrite').parquet('../data/curated/all_details/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = spark.read.parquet('../data/curated/all_details/')\n",
    "\n",
    "# view sample of a random user's transaction history\n",
    "temp.filter(F.col('user_id')==22957).orderBy('dollar_value', ascending=False).limit(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
