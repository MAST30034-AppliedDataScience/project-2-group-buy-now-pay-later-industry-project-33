{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer retention\n",
    "\n",
    "In this notebook, we will generate features that indicate customer retention for merchants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/04 12:37:14 WARN Utils: Your hostname, codespaces-c6855a resolves to a loopback address: 127.0.0.1; using 10.0.0.100 instead (on interface eth0)\n",
      "24/10/04 12:37:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/04 12:37:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/10/04 12:37:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Customer Retention\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"9g\") \n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- dollar_value: double (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- consumer_fraud: double (nullable = true)\n",
      " |-- merchant_fraud: double (nullable = true)\n",
      "\n",
      "14195505\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>order_datetime</th><th>user_id</th><th>dollar_value</th><th>order_id</th><th>postcode</th><th>consumer_fraud</th><th>merchant_fraud</th></tr>\n",
       "<tr><td>80788167198</td><td>2021-08-20</td><td>44</td><td>158.88701712957197</td><td>360a891b-8271-461...</td><td>5074</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>37687523474</td><td>2021-08-20</td><td>44</td><td>14.31708123947184</td><td>5c77cc43-b0fd-432...</td><td>5074</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>43186523025</td><td>2021-08-19</td><td>44</td><td>56.782923459184794</td><td>e75d90e3-2656-4dc...</td><td>5074</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>33233265647</td><td>2021-08-15</td><td>44</td><td>26.557524640642136</td><td>a9f754e4-af51-463...</td><td>5074</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>70172340121</td><td>2021-07-15</td><td>44</td><td>139.3475757940659</td><td>71753d60-5634-422...</td><td>5074</td><td>0.0</td><td>0.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+--------------+-------+------------------+--------------------+--------+--------------+--------------+\n",
       "|merchant_abn|order_datetime|user_id|      dollar_value|            order_id|postcode|consumer_fraud|merchant_fraud|\n",
       "+------------+--------------+-------+------------------+--------------------+--------+--------------+--------------+\n",
       "| 80788167198|    2021-08-20|     44|158.88701712957197|360a891b-8271-461...|    5074|           0.0|           0.0|\n",
       "| 37687523474|    2021-08-20|     44| 14.31708123947184|5c77cc43-b0fd-432...|    5074|           0.0|           0.0|\n",
       "| 43186523025|    2021-08-19|     44|56.782923459184794|e75d90e3-2656-4dc...|    5074|           0.0|           0.0|\n",
       "| 33233265647|    2021-08-15|     44|26.557524640642136|a9f754e4-af51-463...|    5074|           0.0|           0.0|\n",
       "| 70172340121|    2021-07-15|     44| 139.3475757940659|71753d60-5634-422...|    5074|           0.0|           0.0|\n",
       "+------------+--------------+-------+------------------+--------------------+--------+--------------+--------------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_sdf = spark.read.parquet(\"../data/curated/fraud_watch/\")\n",
    "all_data_sdf.printSchema()\n",
    "print(all_data_sdf.count())\n",
    "all_data_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derive necessary information from merchant-customer relationship.\n",
    "1. Calculate median transaction frequency between each customer and merchant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Extract year and month from the 'order_datetime' field\n",
    "all_data_sdf = all_data_sdf.withColumn(\"year_month\", F.date_format(\"order_datetime\", \"yyyy-MM\"))\n",
    "\n",
    "# Step 2: Group by 'merchant_abn', 'user_id', and 'year_month' to get monthly transaction counts\n",
    "monthly_trans_count = all_data_sdf.groupBy(\"merchant_abn\", \"user_id\", \"year_month\") \\\n",
    "    .agg(F.count(\"order_id\").alias(\"monthly_count\"))\n",
    "\n",
    "# Step 3: Calculate the median transaction frequency for each 'merchant_abn' and 'user_id'\n",
    "# We use approx_percentile to get the median (50th percentile)\n",
    "median_trans_freq_sdf = monthly_trans_count.groupBy(\"merchant_abn\", \"user_id\") \\\n",
    "    .agg(F.expr('percentile_approx(monthly_count, 0.5)').alias('median_transaction_frequency'))\n",
    "\n",
    "# Step 4: Show the new dataframe with unique merchant_abn, user_id, and median_transaction_frequency\n",
    "median_trans_freq_sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8516030"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_trans_freq_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate a boolean field that indicates whether a customer is a returning customer for a certain merchant.\n",
    "Returning customer is defined by having more than one transaction with the same merchant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+---------------------+\n",
      "|merchant_abn|user_id|is_returning_customer|\n",
      "+------------+-------+---------------------+\n",
      "| 20985347699|     44|                 true|\n",
      "| 21772962346|     61|                false|\n",
      "| 54272781746|     61|                false|\n",
      "| 72539760068|     61|                false|\n",
      "| 32530636640|    105|                 true|\n",
      "| 59162205130|    123|                false|\n",
      "| 67920961191|    127|                false|\n",
      "| 72231375027|    131|                false|\n",
      "| 94455880010|    132|                false|\n",
      "| 78963299452|    132|                false|\n",
      "| 76819856970|    156|                false|\n",
      "| 38603393734|    156|                false|\n",
      "| 60956456424|    160|                 true|\n",
      "| 96152467973|    160|                 true|\n",
      "| 90087872851|    210|                false|\n",
      "| 87630626808|    218|                false|\n",
      "| 32361057556|    218|                 true|\n",
      "| 76646764782|    218|                false|\n",
      "| 74648589246|    218|                false|\n",
      "| 62773208456|    218|                false|\n",
      "+------------+-------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'merchant_abn' and 'user_id' to count transactions\n",
    "customer_transaction_count = all_data_sdf.groupBy(\"merchant_abn\", \"user_id\") \\\n",
    "    .agg(F.count(\"order_id\").alias(\"transaction_count\"))\n",
    "\n",
    "# Step 2: Create a boolean column indicating if the customer is returning (more than 1 transaction)\n",
    "returning_customer_sdf = customer_transaction_count.withColumn(\n",
    "    \"is_returning_customer\", F.when(F.col(\"transaction_count\") > 1, True).otherwise(False)\n",
    ")\n",
    "\n",
    "# Step 3: Show the resulting DataFrame\n",
    "returning_customer_sdf.select(\"merchant_abn\", \"user_id\", \"is_returning_customer\").show()\n",
    "\n",
    "returning_customer_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8516030"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returning_customer_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find median transaction value between each merchant and customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'merchant_abn' and 'user_id', then calculate the median transaction value\n",
    "median_transaction_value_sdf = all_data_sdf.groupBy(\"merchant_abn\", \"user_id\") \\\n",
    "    .agg(F.expr('percentile_approx(dollar_value, 0.5)').alias('median_transaction_value'))\n",
    "\n",
    "# Step 2: Show the resulting DataFrame with merchant_abn, user_id, and median transaction value\n",
    "median_transaction_value_sdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join these new columns together, on key (merchant_abn, user_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Using the following dataframes:\n",
    "# 1. median_trans_freq_sdf: Contains 'merchant_abn', 'user_id', and 'median_transaction_frequency'\n",
    "# 2. median_transaction_value_sdf: Contains 'merchant_abn', 'user_id', and 'median_transaction_value'\n",
    "# 3. returning_customer_sdf: Contains 'merchant_abn', 'user_id', and 'is_returning_customer'\n",
    "\n",
    "# Step 1: Join median transaction frequency with median transaction value\n",
    "merchant_customer_sdf = median_trans_freq_sdf.join(\n",
    "    median_transaction_value_sdf,\n",
    "    on=[\"merchant_abn\", \"user_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 2: Join the result with the returning customer DataFrame\n",
    "merchant_customer_sdf = merchant_customer_sdf.join(\n",
    "    returning_customer_sdf,\n",
    "    on=[\"merchant_abn\", \"user_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: Show the final result\n",
    "merchant_customer_sdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have information for merchant-customer relationship on:\n",
    "1. median_transaction_frequency\n",
    "2. is_returning_customer\n",
    "3. median_transaction_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some visualisation to check for reasonableness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a 1% sample of the Spark DataFrame\n",
    "merchant_customer_sample_sdf = merchant_customer_sdf.sample(fraction=0.0001, seed=42)\n",
    "\n",
    "# Convert the sampled Spark DataFrame to Pandas DataFrame\n",
    "merchant_customer_pdf = merchant_customer_sample_sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median Transaction Frequency Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(merchant_customer_pdf['median_transaction_frequency'], bins=10, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Median Transaction Frequency')\n",
    "plt.xlabel('Median Transaction Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Median Transaction Value Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(merchant_customer_pdf['median_transaction_value'], bins=10, color='green', edgecolor='black')\n",
    "plt.title('Distribution of Median Transaction Value')\n",
    "plt.xlabel('Median Transaction Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Is Returning Customer Distribution (Boolean Distribution)\n",
    "plt.figure(figsize=(10, 5))\n",
    "merchant_customer_pdf['is_returning_customer'].value_counts().plot(kind='bar', color='orange', edgecolor='black')\n",
    "plt.title('Distribution of Returning Customers')\n",
    "plt.xlabel('Is Returning Customer')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['True', 'False'], rotation=0)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aggregate information by merchant\n",
    "\n",
    "1. Count number of customers per merchant.\n",
    "2. Find proprotion of returning customers per merchant.\n",
    "3. Find median of median monthly customer transaction frequencies.\n",
    "4. Find median of median customer transaction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate number of customers, returning customer proportion, and medians\n",
    "merchant_aggregated_sdf = merchant_customer_sdf.groupBy(\"merchant_abn\").agg(\n",
    "    # Number of unique customers\n",
    "    F.countDistinct(\"user_id\").alias(\"number_of_customers\"),\n",
    "    \n",
    "    # Returning customer proportion\n",
    "    (F.sum(F.when(F.col(\"is_returning_customer\") == True, 1).otherwise(0)) / \n",
    "     F.countDistinct(\"user_id\")).alias(\"returning_customer_proportion\"),\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "merchant_aggregated_sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merchant_aggregated_sdf.write.mode('overwrite').parquet(\"../data/curated/customer_retention/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_two_aggregated_sdf = merchant_customer_sdf.groupBy(\"merchant_abn\").agg(\n",
    "    # Median transaction frequency\n",
    "    F.expr('percentile_approx(median_transaction_frequency, 0.5)').alias('median_transaction_frequency'),\n",
    "    \n",
    "    # Median transaction value\n",
    "    F.expr('percentile_approx(median_transaction_value, 0.5)').alias('median_transaction_value'),\n",
    ")\n",
    "merchant_two_aggregated_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_aggregated_sdf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
