{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer retention\n",
    "\n",
    "In this notebook, we will generate features that indicate customer retention for merchants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Customer Retention\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"8g\") \n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_sdf = spark.read.parquet(\"../data/curated/fraud_watch/\")\n",
    "all_data_sdf.printSchema()\n",
    "print(all_data_sdf.count())\n",
    "all_data_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derive necessary information from merchant-customer relationship.\n",
    "1. Calculate median transaction frequency between each customer and merchant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Extract year and month from the 'order_datetime' field\n",
    "all_data_sdf = all_data_sdf.withColumn(\"year_month\", F.date_format(\"order_datetime\", \"yyyy-MM\"))\n",
    "\n",
    "# Step 2: Group by 'merchant_abn', 'user_id', and 'year_month' to get monthly transaction counts\n",
    "monthly_trans_count = all_data_sdf.groupBy(\"merchant_abn\", \"user_id\", \"year_month\") \\\n",
    "    .agg(F.count(\"order_id\").alias(\"monthly_count\"))\n",
    "\n",
    "# Step 3: Calculate the median transaction frequency for each 'merchant_abn' and 'user_id'\n",
    "# We use approx_percentile to get the median (50th percentile)\n",
    "median_trans_freq_sdf = monthly_trans_count.groupBy(\"merchant_abn\", \"user_id\") \\\n",
    "    .agg(F.expr('percentile_approx(monthly_count, 0.5)').alias('median_transaction_frequency'))\n",
    "\n",
    "# Step 4: Show the new dataframe with unique merchant_abn, user_id, and median_transaction_frequency\n",
    "median_trans_freq_sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_trans_freq_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate a boolean field that indicates whether a customer is a returning customer for a certain merchant.\n",
    "Returning customer is defined by having more than one transaction with the same merchant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'merchant_abn' and 'user_id' to count transactions\n",
    "customer_transaction_count = all_data_sdf.groupBy(\"merchant_abn\", \"user_id\") \\\n",
    "    .agg(F.count(\"order_id\").alias(\"transaction_count\"))\n",
    "\n",
    "# Step 2: Create a boolean column indicating if the customer is returning (more than 1 transaction)\n",
    "returning_customer_sdf = customer_transaction_count.withColumn(\n",
    "    \"is_returning_customer\", F.when(F.col(\"transaction_count\") > 1, True).otherwise(False)\n",
    ")\n",
    "\n",
    "# Step 3: Show the resulting DataFrame\n",
    "returning_customer_sdf.select(\"merchant_abn\", \"user_id\", \"is_returning_customer\").show()\n",
    "\n",
    "returning_customer_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returning_customer_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find median transaction value between each merchant and customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Group by 'merchant_abn' and 'user_id', then calculate the median transaction value\n",
    "median_transaction_value_sdf = all_data_sdf.groupBy(\"merchant_abn\", \"user_id\") \\\n",
    "    .agg(F.expr('percentile_approx(dollar_value, 0.5)').alias('median_transaction_value'))\n",
    "\n",
    "# Step 2: Show the resulting DataFrame with merchant_abn, user_id, and median transaction value\n",
    "median_transaction_value_sdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join these new columns together, on key (merchant_abn, user_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Using the following dataframes:\n",
    "# 1. median_trans_freq_sdf: Contains 'merchant_abn', 'user_id', and 'median_transaction_frequency'\n",
    "# 2. median_transaction_value_sdf: Contains 'merchant_abn', 'user_id', and 'median_transaction_value'\n",
    "# 3. returning_customer_sdf: Contains 'merchant_abn', 'user_id', and 'is_returning_customer'\n",
    "\n",
    "# Step 1: Join median transaction frequency with median transaction value\n",
    "merchant_customer_sdf = median_trans_freq_sdf.join(\n",
    "    median_transaction_value_sdf,\n",
    "    on=[\"merchant_abn\", \"user_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 2: Join the result with the returning customer DataFrame\n",
    "merchant_customer_sdf = merchant_customer_sdf.join(\n",
    "    returning_customer_sdf,\n",
    "    on=[\"merchant_abn\", \"user_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: Show the final result\n",
    "merchant_customer_sdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have information for merchant-customer relationship on:\n",
    "1. median_transaction_frequency\n",
    "2. is_returning_customer\n",
    "3. median_transaction_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some visualisation to check for reasonableness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes a while to run. \n",
    "\n",
    "# Step 1: Take a 1% sample of the Spark DataFrame (without replacement)\n",
    "merchant_customer_sample_sdf = merchant_customer_sdf.sample(fraction=0.0001, seed=42)\n",
    "\n",
    "# Step 2: Convert the sampled Spark DataFrame to Pandas DataFrame\n",
    "merchant_customer_pdf = merchant_customer_sample_sdf.toPandas()\n",
    "\n",
    "# Step 3: Visualize distributions using the Pandas DataFrame\n",
    "\n",
    "# Median Transaction Frequency Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(merchant_customer_pdf['median_transaction_frequency'], bins=10, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Median Transaction Frequency')\n",
    "plt.xlabel('Median Transaction Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Median Transaction Value Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(merchant_customer_pdf['median_transaction_value'], bins=10, color='green', edgecolor='black')\n",
    "plt.title('Distribution of Median Transaction Value')\n",
    "plt.xlabel('Median Transaction Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Is Returning Customer Distribution (Boolean Distribution)\n",
    "plt.figure(figsize=(10, 5))\n",
    "merchant_customer_pdf['is_returning_customer'].value_counts().plot(kind='bar', color='orange', edgecolor='black')\n",
    "plt.title('Distribution of Returning Customers')\n",
    "plt.xlabel('Is Returning Customer')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['True', 'False'], rotation=0)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aggregate information by merchant\n",
    "\n",
    "1. Count number of customers per merchant.\n",
    "2. Find proprotion of returning customers per merchant.\n",
    "3. Find median of median monthly customer transaction frequencies.\n",
    "4. Find median of median customer transaction value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate number of customers, returning customer proportion, and medians\n",
    "merchant_aggregated_sdf = merchant_customer_sdf.groupBy(\"merchant_abn\").agg(\n",
    "    # Number of unique customers\n",
    "    F.countDistinct(\"user_id\").alias(\"number_of_customers\"),\n",
    "    \n",
    "    # Returning customer proportion\n",
    "    (F.sum(F.when(F.col(\"is_returning_customer\") == True, 1).otherwise(0)) / \n",
    "     F.countDistinct(\"user_id\")).alias(\"returning_customer_proportion\"),\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "merchant_aggregated_sdf.show()\n",
    "merchant_aggregated_sdf.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_aggregated_sdf.write.mode(\"overwrite\").parquet(\"../data/curated/customer_retention/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_two_aggregated_sdf = merchant_customer_sdf.groupBy(\"merchant_abn\").agg(\n",
    "    # Median transaction frequency\n",
    "    F.expr('percentile_approx(median_transaction_frequency, 0.5)').alias('median_transaction_frequency'),\n",
    "    \n",
    "    # Median transaction value\n",
    "    F.expr('percentile_approx(median_transaction_value, 0.5)').alias('median_transaction_value'),\n",
    ")\n",
    "merchant_two_aggregated_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_aggregated_sdf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
