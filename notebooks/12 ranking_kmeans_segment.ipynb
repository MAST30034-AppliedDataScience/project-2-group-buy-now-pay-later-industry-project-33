{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmean segment ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Categories\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"10g\") \n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.network.timeout\", \"600s\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, PCA, StopWordsRemover\n",
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant = spark.read.parquet(\"../data/curated/part_1/clean_merchant.parquet\")\n",
    "goods = merchant.select(\"goods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clean the 'goods' column by removing punctuation and converting to lowercase\n",
    "clean_goods = goods.withColumn(\"str_goods\", lower(regexp_replace(\"goods\", \"[^\\w\\s]\", \"\")))\n",
    "\n",
    "# Step 2: Tokenize the cleaned 'str_goods' column\n",
    "tokenizer = Tokenizer(inputCol=\"str_goods\", outputCol=\"tokens\")\n",
    "clean_goods = tokenizer.transform(clean_goods)\n",
    "\n",
    "# Step 3: Get default stop words from StopWordsRemover\n",
    "default_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "# Step 4: Add custom stop words to the list\n",
    "additional_stopwords = ['new', 'shops', 'supplies', 'parts', 'services', '', \n",
    "                        'supply', 'integrated', 'equipment', 'sales', \n",
    "                        'dealers', 'restoration']\n",
    "all_stopwords = default_stopwords + additional_stopwords\n",
    "\n",
    "# Step 5: Remove stop words, including the additional custom words\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_goods\", stopWords=all_stopwords)\n",
    "clean_goods = remover.transform(clean_goods)\n",
    "\n",
    "# Step 6: Select only the 'clean_goods' column\n",
    "clean_goods = clean_goods.select('clean_goods')\n",
    "\n",
    "# Show the results\n",
    "clean_goods.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_goods.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an index to both DataFrames to ensure they align\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add an index column to df1\n",
    "df1_with_index = merchant.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Add an index column to df2\n",
    "df2_with_index = clean_goods.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join the DataFrames on the index column\n",
    "df_combined = df1_with_index.join(df2_with_index, on=\"index\", how=\"inner\").drop(\"index\")\n",
    "\n",
    "# Show the combined DataFrame\n",
    "df_combined.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Step 1: Initialize the Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=10, inputCol=\"clean_goods\", outputCol=\"clean_goods_word2vec\")\n",
    "\n",
    "# Step 2: Train the Word2Vec model\n",
    "model = word2vec.fit(df_combined)\n",
    "\n",
    "# Step 3: Transform the 'clean_goods' column into word vectors\n",
    "df_word2vec = model.transform(df_combined)\n",
    "\n",
    "# Show the resulting DataFrame with word vectors\n",
    "df_word2vec.select(\"clean_goods\", \"clean_goods_word2vec\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Step 1: Create a list to store the SSE values for each k\n",
    "sse = []\n",
    "\n",
    "# Step 2: Test K-Means with different values of k (e.g., from 2 to 10 clusters)\n",
    "for k in range(2, 16):\n",
    "    kmeans = KMeans(featuresCol=\"clean_goods_word2vec\", predictionCol=\"cluster\", k=k)\n",
    "    kmeans_model = kmeans.fit(df_word2vec)\n",
    "    \n",
    "    # Compute the Sum of Squared Errors (SSE) and append to the list\n",
    "    sse.append(kmeans_model.summary.trainingCost)\n",
    "\n",
    "# Step 3: Plot the SSE against k to find the \"elbow\" point\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(2, 16), sse, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum of Squared Errors (SSE)')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"clean_goods_word2vec\", predictionCol=\"cluster\", k=9)  # k is the number of clusters\n",
    "kmeans_model = kmeans.fit(df_word2vec)\n",
    "\n",
    "# Make predictions (assign clusters to the data points)\n",
    "df_clusters = kmeans_model.transform(df_word2vec)\n",
    "\n",
    "# Show the resulting DataFrame with clusters\n",
    "df_clusters.select(\"clean_goods\", \"clean_goods_word2vec\", \"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, desc, count\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Step 1: Explode the 'clean_goods' column to individual clean_good entries\n",
    "df_exploded = df_clusters.withColumn(\"clean_good\", explode(col(\"clean_goods\")))\n",
    "\n",
    "# Step 2: Group by 'cluster' and 'clean_good', then count occurrences of each clean_good\n",
    "df_grouped = df_exploded.groupBy(\"cluster\", \"clean_good\").count()\n",
    "\n",
    "# Step 3: Rank clean_goods within each cluster based on count\n",
    "window = Window.partitionBy(\"cluster\").orderBy(desc(\"count\"))\n",
    "df_ranked = df_grouped.withColumn(\"rank\", row_number().over(window))\n",
    "\n",
    "# Step 4: Filter to keep only the most common clean_good per cluster\n",
    "df_most_common_good = df_ranked.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "# Step 5: Show the most common clean_good for each cluster\n",
    "df_most_common_good.select(\"cluster\", \"clean_good\", \"count\").show(truncate=False)\n",
    "\n",
    "# Step 6: Count the number of records (merchants) in each cluster\n",
    "df_cluster_count = df_clusters.groupBy(\"cluster\").agg(count(\"*\").alias(\"merchant_count\"))\n",
    "\n",
    "# Step 7: Show the number of records/merchants in each cluster\n",
    "df_cluster_count.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_common_good.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters.filter(col('cluster')==3).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Define the mapping of clusters to segments\n",
    "df_segmented = df_clusters.withColumn(\n",
    "    \"segment\",\n",
    "    when(col(\"cluster\") == 0, \"Entertainment & Media\") #music\n",
    "    .when(col(\"cluster\") == 1, \"Office & Home Supplies\") #office\n",
    "    .when(col(\"cluster\") == 2, \"Office & Home Supplies\") #furniture\n",
    "    .when(col(\"cluster\") == 3, \"Miscellaneous\") #novelty\n",
    "    .when(col(\"cluster\") == 4, \"Fashion\") #jewelry\n",
    "    .when(col(\"cluster\") == 5, \"Entertainment & Media\") #television\n",
    "    .when(col(\"cluster\") == 6, \"Miscellaneous\") #shoe\n",
    "    .when(col(\"cluster\") == 7, \"Miscellaneous\") #craft\n",
    "    .when(col(\"cluster\") == 8, \"Technology\") #computer\n",
    ")\n",
    "\n",
    "# Show the DataFrame with the assigned segments\n",
    "df_segmented.select(\"cluster\", \"clean_goods\", \"segment\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segmented.select(\"segment\", \"merchant_abn\", \"goods\").write.mode('overwrite').parquet(\"../data/curated/clean_merchant_segmented.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking with respect to each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ranking = spark.read.parquet(\"../data/curated/merchant_ranking\")\n",
    "merchant_segment = spark.read.parquet(\"../data/curated/clean_merchant_segmented.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join each merchant in initial ranking by segment\n",
    "merchant_segment_ranking = initial_ranking.join(merchant_segment, how='left', on='merchant_abn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segmented.select(\"segment\", \"merchant_abn\", \"goods\").write.mode('overwrite').parquet(\"../data/curated/clean_merchant_segmented.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking with respect to each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ranking = spark.read.parquet(\"../data/curated/merchant_ranking\")\n",
    "merchant_segment = spark.read.parquet(\"../data/curated/clean_merchant_segmented.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join each merchant in initial ranking by segment\n",
    "merchant_segment_ranking = initial_ranking.join(merchant_segment, how='left', on='merchant_abn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_segment_ranking.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [\n",
    "    \"Entertainment & Media\",\n",
    "    \"Office & Home Supplies\",\n",
    "    \"Miscellaneous\",\n",
    "    \"Fashion\",\n",
    "    \"Technology\"\n",
    "]\n",
    "\n",
    "for segment in segments:\n",
    "    print(segment)\n",
    "    segment_ranking = merchant_segment_ranking.filter(F.col('segment') == segment)\n",
    "    segment_ranking.orderBy(F.col('final_score').desc()).show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
