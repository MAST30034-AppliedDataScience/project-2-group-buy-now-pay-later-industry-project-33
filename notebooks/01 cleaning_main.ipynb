{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa2f251-b7f9-4473-869b-6a1874419323",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "In this session, we will do same basic cleaning steps on the main datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df2ebf-e754-4efd-b9d6-15e7748fa5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import shapefile as shp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6152431-6b96-4c2d-b79e-22b3bc657fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Data Cleaning\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"9g\") \n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29780e6-5c75-447b-a13a-6e77f1806d73",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a387a-0e5d-4873-9673-7415e1079766",
   "metadata": {},
   "source": [
    "We have 3 kinds of dataset: transactions, consumer, merchant. We aim to find top 100 merchants we should accept, so, let's look at merchant dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3cae1-7821-42cd-a2e2-44033145ff07",
   "metadata": {},
   "source": [
    "## 1. Merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad33cc5-9a6d-45ee-b362-ee16dc5e01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on merchants\n",
    "merchant = spark.read.parquet(\"../data/tables/part_1/tbl_merchants.parquet\")\n",
    "\n",
    "# Information on merchant's fraud probability\n",
    "merchant_fraud_prob = pd.read_csv(\"../data/tables/part_1/merchant_fraud_probability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168e491-d52c-46b7-8710-bba84dc88a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {merchant.count()}\")\n",
    "merchant.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35421c-5aff-4b96-91e8-0e6d6b7c53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_fraud_prob.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86201f-ba7a-40fd-860e-11953eec3062",
   "metadata": {},
   "source": [
    "We should look at the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e1520-7aa5-454c-9dae-9a082918d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd746f8-5f6f-43d3-8247-a7d8f4f628dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_fraud_prob.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c09d57-127d-411a-b9b0-4044f72fde4e",
   "metadata": {},
   "source": [
    "### Comment\n",
    "- The merchant dataset contains their names, tags (seems like stuff they sell), and the abn.\n",
    "- For the probability dataset, they record the datetime of an order and the probability it is broken?.\n",
    "- Two tables share the `merchant_abn` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95e06a-1935-4687-a29b-5a2399019e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the tags column\n",
    "merchant.select(\"tags\").limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce280d1c-73a9-4954-910c-f52f68e85f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicate\n",
    "print(f\"Number of duplicates: {merchant.select('merchant_abn').count() - merchant.select('merchant_abn').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc188f1-fc15-4ecc-9e06-73722bd8613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null value\n",
    "print(f\"NaN in merchant detail: {merchant.filter(F.col('merchant_abn').isNull() | F.col('tags').isNull() | F.col('name').isNull()).count()}\")\n",
    "print(f\"Nan in merchant fraud rate:\\n{merchant_fraud_prob.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a5a72b-1e58-4c05-ab3a-d97c0ed1c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of merchant with a fraud rate\n",
    "print(f\"Number of merchant with a fraud rate: {len(merchant_fraud_prob.merchant_abn.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3276a-06f8-44b8-8e9b-ac1122223ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The abn with fraud but not in  merchant info\n",
    "abn_not_in_merchant = spark.createDataFrame(pd.DataFrame(merchant_fraud_prob.merchant_abn)).subtract(merchant.select('merchant_abn'))\n",
    "abn_not_in_merchant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c881e9e-b913-467d-ab7a-0aa55adf63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of abn not in merchant dataset: {abn_not_in_merchant.count()}\")\n",
    "merchant.filter(F.col(\"merchant_abn\") == 82999039227).show()\n",
    "merchant_fraud_prob[merchant_fraud_prob.merchant_abn == 82999039227]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8f4a3-6cb5-40d7-88c7-1b505c809894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time range for fraud probability\n",
    "merchant_fraud_prob.loc[:,\"order_datetime\"] = pd.to_datetime(merchant_fraud_prob.order_datetime)\n",
    "merchant_fraud_prob.order_datetime.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41815e-9b69-4b18-8275-4842169b6715",
   "metadata": {},
   "source": [
    "The time range recoded in the merchant fraud dataset is from 25/03/2021 to 27/02/2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ab721-1c60-41b9-9247-f18979b7e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if fraud prob is in a valid range\n",
    "merchant_fraud_prob.fraud_probability.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964a6a2-32d1-4926-a6e5-206b9c991659",
   "metadata": {},
   "source": [
    "The fraud probability is in good range (from 0 to 1), with the highest probability at 94%, which was found to belong to an abn not existing in the merchant dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32342d6e-9a87-4a8d-aab3-a3775cc79d78",
   "metadata": {},
   "source": [
    "### Consideration\n",
    "- We need to separate each tupe into 3 values: stuff_type, ?, take_rate\n",
    "- How to deal with the text containing goods: remove stop words, dec2vec, count2vec, etc.\n",
    "- Convert `order_datetime` to datetime data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b728485-8e27-4415-a18a-d181716edf00",
   "metadata": {},
   "source": [
    "## 2. Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb6a72e-72b6-4cb5-ae43-3fe07f09ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on consumer\n",
    "consumer_user_detail = spark.read.parquet(\"../data/tables/part_1/consumer_user_details.parquet\")\n",
    "consumer = pd.read_csv(\"../data/tables/part_1/tbl_consumer.csv\", delimiter=\"|\")\n",
    "\n",
    "# Information on consumer's fraud probability\n",
    "consumer_fraud_prob = pd.read_csv(\"../data/tables/part_1/consumer_fraud_probability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95319511-dec7-4570-97e2-10eeb1a58d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shapes: {consumer.shape}\")\n",
    "consumer.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b2ac4-4de2-4c3a-accb-cd3d3a11ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {consumer_fraud_prob.shape}\")\n",
    "consumer_fraud_prob.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4baf4fc-30e4-423f-93cc-824ae68891c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {consumer_user_detail.count()}\")\n",
    "consumer_user_detail.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7767dae-6a0b-4fff-af4e-9b31331527ff",
   "metadata": {},
   "source": [
    "### Comment\n",
    "- The consumer dataset contains their name, address, state, postcode, gender and consumer_id. While the consumer user details dataset contain corresponding consumer_id for each user_id.\n",
    "- Consumer fraud probability and consumer dataset can be merged through consumer user detail dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db740d14-0262-40b2-853c-caffd59fbb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicate in user_id and consumer_id\n",
    "print(f\"Number of duplicates in user_id: {consumer_user_detail.select('user_id').count() - consumer_user_detail.select('user_id').distinct().count()}\")\n",
    "\n",
    "print(f\"Number of duplicates in consumer_id: {consumer_user_detail.select('consumer_id').count() - consumer_user_detail.select('consumer_id').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f683b-f37b-4a0d-bb0f-47b604135b31",
   "metadata": {},
   "source": [
    "**Question**: Why do we need both consumer_id and user_id (related to database)?\n",
    "\n",
    "An user id has to be in a range of 1 and 499,999, while consumer_id can be random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627bdf1-86ed-4694-b0ae-4c66c11efc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "print(f\"Nan in consumer fraud rate:\\n{consumer_fraud_prob.isna().sum()}\")\n",
    "print(f\"Nan in consumer fraud rate:\\n{consumer.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24afb9d-5a37-4493-ad29-82dc435168e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if user id range in other dataframes is valid.\n",
    "print(f\"Max user id in the fraud record: {max(consumer_fraud_prob.user_id)}\")\n",
    "print(f\"Min user id in the fraud record: {min(consumer_fraud_prob.user_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733d69e-e942-4321-8a4f-784ccb26308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the consumer id without a valid corresponding user id\n",
    "invalid_consumer_id = spark.createDataFrame(pd.DataFrame(consumer.consumer_id)).subtract(consumer_user_detail.select('consumer_id'))\n",
    "invalid_consumer_id.limit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba109c6-eea9-4797-888e-42b814d40b08",
   "metadata": {},
   "source": [
    "There are no problems in consumer_id and user_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce83d6a-239b-4c4b-8d76-c9a4c5dedaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time range for fraud probability\n",
    "consumer_fraud_prob.loc[:,\"order_datetime\"] = pd.to_datetime(consumer_fraud_prob.order_datetime)\n",
    "consumer_fraud_prob.order_datetime.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4dde1-90d4-45c9-94f6-5de89b6453ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if fraud prob is in a valid range\n",
    "consumer_fraud_prob.fraud_probability.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774752e-bf78-426e-817d-a2052370b9c9",
   "metadata": {},
   "source": [
    "It has a suitable time range and probability. Overall, we just need to convert order_datetime to datetime data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35f45f-e2c1-4a14-b8c5-bf82548b3226",
   "metadata": {},
   "source": [
    "## 3. Transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188d57e-dc32-4419-9684-48cc6fa83cbe",
   "metadata": {},
   "source": [
    "This dataset contains details for each transaction between a merchant and a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90613fab-112c-4bb7-9194-981e6d6d8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transaction dataset\n",
    "transaction1 = spark.read.parquet(\"../data/tables/part_2/*\")\n",
    "transaction2 = spark.read.parquet(\"../data/tables/part_3/*\")\n",
    "transaction3 = spark.read.parquet(\"../data/tables/part_4/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3695d7-1010-48c5-bd6b-9f6098c7579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets into a single DataFrame\n",
    "transaction = transaction1.union(transaction2).union(transaction3)\n",
    "transaction.count() # Number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5607c0-14ad-4ecb-9c52-904f4e61899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3c031-f0a0-4102-ac54-a7a1744e1386",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5e734-fd6e-44b3-be63-750f4be9ac90",
   "metadata": {},
   "source": [
    "#### 1. Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda306c-bd92-4001-9ea8-42346a4b5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check\n",
    "cols = transaction.columns\n",
    "\n",
    "for col_name in cols:\n",
    "    # Filter rows where values are null and count them\n",
    "    null_count = transaction.filter(F.col(col_name).isNull()).count()\n",
    "\n",
    "    if null_count > 0:\n",
    "        print(f\"Number of rows with null {col_name}: {null_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad93b4-cdac-4835-afc5-084b5f65de8f",
   "metadata": {},
   "source": [
    "So there are no missing values in any columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc4d5a-2ac3-4744-b47c-d0029326c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['user_id', 'dollar_value', 'order_datetime']:\n",
    "    print(f\"Max of {col}: {transaction.agg({col: 'max'})}\")\n",
    "    print(f\"Min of {col}: {transaction.agg({col: 'min'})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdc78a-8de3-4725-a5bf-dbdecc6a245d",
   "metadata": {},
   "source": [
    "### Comment\n",
    "- Although we have 499,999 user ids, but only at most 24081 of them made a transaction within the provided time range.\n",
    "- the `dollar_value` range seems strange as the minimum value is almost 0. We may need to do some outlier analysis for it.\n",
    "- The time range of the transaction dataset is wider than the time range of other datasets, which can lead to missing data when we joined every table together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56c9f0-ad27-4cf6-b7b0-0f01793007a9",
   "metadata": {},
   "source": [
    "# Create Cleaning Function\n",
    "\n",
    "At this step, only the column `tags` of merchant dataset needs to be preprocessed. We decide to convert the tbl_merchants.parquet to csv because of its small size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4db9c223-e0a3-47ec-b600-fb82c0d02a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from etl import clean_merchant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f6e92-c278-420a-90ce-e4774a2c513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clean function\n",
    "merchant = spark.read.parquet(\"../data/tables/part_1/tbl_merchants.parquet\")\n",
    "merchant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47be1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_merchant_df(merchant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb312607-a3ca-4f88-8a92-e7221ab0a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_merchant_df = spark.read.parquet(\"../data/curated/part_1/tbl_merchants.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845118d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_merchant_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c986b8-4f4c-4340-af79-8e1443479c16",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f0db046-90de-48ff-81db-d05c1500d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregrated_trans_df = transaction.groupBy(\"merchant_abn\").agg(F.sum(\"dollar_value\"), F.count(\"dollar_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecfc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregrated_trans_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ea6b6",
   "metadata": {},
   "source": [
    "Creating a scaled revenue as as feature, we take log of the ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c51776f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregrated_trans_df = aggregrated_trans_df.withColumn(\"log_ratio\", F.log(F.col(\"sum(dollar_value)\") / F.col(\"count(dollar_value)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b59d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregrated_trans_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1aa65651",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_merchant_df = cleaned_merchant_df.join(aggregrated_trans_df, on=\"merchant_abn\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94b83b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_merchant_df = cleaned_merchant_df.withColumn(\"unscaled_earning\", (F.col(\"take_rate\")/100 * F.col(\"sum(dollar_value)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f157d-0c2e-4819-bf0b-00ceed029765",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_merchant_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_merchant_df.write.mode('overwrite').parquet('../data/curated/part_1/clean_merchant.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ddedb-3248-4c5c-bfee-1c544a6c7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_merchant_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d24690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
