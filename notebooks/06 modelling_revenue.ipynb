{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling: monthly revenue\n",
    "In this notebook, we aim to model an additional feature for the future revenue of merchants based on the most recent 12 months of transactions. We test Linear Regression and Random Forest models, accounting for monthly seasonal changes as well as a variable model parameters for merchants in different revenue bands.\n",
    "\n",
    "This becomes one of the 5 key metrics used to rank merchants for the BNPL scheme (predicted revenue growth rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Modelling monthly revenue\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"16G\")\n",
    "    .config(\"spark.executor.memory\", \"16G\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"64MB\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.network.timeout\", \"600s\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = spark.read.parquet('../data/curated/all_details/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by month and merchant, aggregating the sum of dollar_value and the count of transactions\n",
    "merchant_monthly = transactions.groupBy(F.date_format(F.col('order_datetime'), 'yyyy-MM'), 'merchant_abn').agg(F.sum('dollar_value'))\n",
    "count_transactions = transactions.groupBy(F.date_format(F.col('order_datetime'), 'yyyy-MM'), 'merchant_abn').agg(F.count('dollar_value'))\n",
    "merchant_monthly = merchant_monthly.join(count_transactions, ['date_format(order_datetime, yyyy-MM)', 'merchant_abn'], how='inner')\n",
    "merchant_monthly = merchant_monthly.withColumnRenamed('sum(dollar_value)', 'monthly_revenue')\n",
    "merchant_monthly = merchant_monthly.withColumnRenamed('count(dollar_value)', 'number_transactions')\n",
    "merchant_monthly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant = spark.read.parquet('../data/curated/part_1/clean_merchant.parquet')\n",
    "merchant = merchant.withColumn('average_revenue', F.col('sum(dollar_value)') / F.col('count(dollar_value)'))\n",
    "merchant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_with_revenue = merchant.join(merchant_monthly, on ='merchant_abn', how='left')\n",
    "omitted_merchants = merchant.join(merchant_monthly, on ='merchant_abn', how='left_anti')\n",
    "omitted_merchants.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_with_revenue = merchant_with_revenue.withColumnRenamed('sum(dollar_value)', 'total_revenue').withColumnRenamed('date_format(order_datetime, yyyy-MM)', 'month_year')\n",
    "merchant_with_revenue.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a month column\n",
    "merchant_with_revenue = merchant_with_revenue.withColumn('month', F.month(F.col('month_year')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_with_revenue.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing window \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Creating a month since first transaction column for each merchant\n",
    "merchant_with_revenue = merchant_with_revenue.withColumn('first_transaction', F.min('month_year').over(Window.partitionBy('merchant_abn')))\n",
    "merchant_with_revenue = merchant_with_revenue.withColumn('month_since_first_transaction', F.months_between(F.col('month_year'), F.col('first_transaction')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_with_revenue.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating montly earning column\n",
    "merchant_with_revenue = merchant_with_revenue.withColumn('monthly_earning', F.col('monthly_revenue') * F.col('take_rate')/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_with_revenue.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sin and cos columns for month\n",
    "merchant_with_revenue = merchant_with_revenue.withColumn('month_sin', F.sin(2 * 3.14 * F.col('month') / 12))\n",
    "merchant_with_revenue = merchant_with_revenue.withColumn('month_cos', F.cos(2 * 3.14 * F.col('month') / 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "\n",
    "# Check numerical columns\n",
    "numerical_columns = ['take_rate', 'total_revenue', 'count(dollar_value)', 'log_ratio', 'unscaled_earning', 'average_revenue', 'monthly_revenue', 'number_transactions', 'month_since_first_transaction', 'monthly_earning']\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame for correlation analysis\n",
    "merchant_with_revenue_pd = merchant_with_revenue.select(numerical_columns).toPandas()\n",
    "\n",
    "# Calculate the correlation matrix for the numerical columns\n",
    "corr_df = merchant_with_revenue_pd.corr()\n",
    "\n",
    "# Plot the heatmap using Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Features Correlation Heatmap')\n",
    "plt.savefig(\"../plots/Features_Correlation_Heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the data into training and testing\n",
    "train = merchant_with_revenue.filter(F.col('month_year') < '2022-07')\n",
    "test = merchant_with_revenue.filter(F.col('month_year') >= '2022-07')\n",
    "\n",
    "revenue_levels = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "# Setting up the pipeline for linear regression\n",
    "assembler = VectorAssembler(inputCols=[ 'month_since_first_transaction', 'total_revenue', 'count(dollar_value)', 'month_sin', 'month_cos'], outputCol='features')\n",
    "lr = LinearRegression(featuresCol='features', labelCol='monthly_revenue', elasticNetParam=1)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator \n",
    "evaluator = RegressionEvaluator(labelCol='monthly_revenue', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# Fit the model using CrossValidator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for level in revenue_levels:\n",
    "    # Filter data for the current revenue level\n",
    "    level_data = train.filter(F.col('revenue_level') == level)\n",
    "    # Train the model for the revenue level\n",
    "    model = crossval.fit(level_data)\n",
    "    # Store the model\n",
    "    models[level] = model\n",
    "    # save the model to disk\n",
    "    model.bestModel.save(f'../models/revenue_linear_regression_{level}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in revenue_levels:\n",
    "    # load the model\n",
    "    best_model = PipelineModel.load(f'../models/revenue_linear_regression_{level}')\n",
    "    test_data = test.filter(F.col('revenue_level') == level)\n",
    "    predictions = best_model.transform(test_data)\n",
    "    \n",
    "    # Apply the inverse log (exponentiate) to get the actual predictions for monthly revenue\n",
    "    predictions = predictions.withColumn('predicted_monthly_revenue', F.exp(F.col('prediction')))\n",
    "    \n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"RMSE for revenue level {level}: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar random forest model but for log-transformed revenue\n",
    "pipeline_rf = Pipeline(stages=[assembler, RandomForestRegressor(featuresCol='features', labelCol='monthly_revenue')])\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(RandomForestRegressor.numTrees, [10, 20]) \\\n",
    "    .addGrid(RandomForestRegressor.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_rf = CrossValidator(estimator=pipeline_rf,\n",
    "                            estimatorParamMaps=paramGrid_rf,\n",
    "                            evaluator=evaluator,\n",
    "                            numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for level in revenue_levels:\n",
    "    # Filter data for the current revenue level\n",
    "    level_data = train.filter(F.col('revenue_level') == level)\n",
    "    # Train the model for the revenue level\n",
    "    model = crossval_rf.fit(level_data)\n",
    "    # Store the model\n",
    "    models[level] = model\n",
    "    # save the model to disk\n",
    "    model.bestModel.save(f'../models/revenue_random_forest_{level}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in revenue_levels:\n",
    "    # load the model\n",
    "    best_model = PipelineModel.load(f'../models/revenue_random_forest_{level}')\n",
    "    test_data = test.filter(F.col('revenue_level') == level)\n",
    "    predictions = best_model.transform(test_data)\n",
    "    \n",
    "    # Apply the inverse log to get actual monthly revenue predictions\n",
    "    predictions = predictions.withColumn('predicted_monthly_revenue', F.exp(F.col('prediction')))\n",
    "    \n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"RMSE for revenue level {level}: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now getting the predictions for all merchants for the next year (2023-01 to 2023-12)\n",
    "# Total Revenue, take rate and 'count(dollar_value)' i.e. number of total transactions are same.\n",
    "# We can use the same model for all merchants.\n",
    "# We will create a new dataframe with all the months from 2023-01 to 2023-12 and all the merchants and then predict the monthly revenue for each merchant for each month.\n",
    "# We know months since first transaction will be 24 to 35 for january 2023 to december 2023 respectively.\n",
    "\n",
    "# Create a dataframe with all the months from 2023-01 to 2023-12\n",
    "months = ['2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06', '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12']\n",
    "months_df = spark.createDataFrame([(month,) for month in months], ['month_year'])\n",
    "\n",
    "# Adding all merchants to the dataframe\n",
    "all_merchants = merchant.select('merchant_abn').distinct()\n",
    "months_df = months_df.crossJoin(all_merchants)\n",
    "\n",
    "# Adding the total revenue, take rate and count(dollar_value) columns\n",
    "months_df = months_df.join(merchant, on='merchant_abn', how='left')\n",
    "\n",
    "# Adding the month column\n",
    "months_df = months_df.withColumn('month', F.month(F.col('month_year')))\n",
    "months_df = months_df.withColumn('month_sin', F.sin(2 * 3.14 * F.col('month') / 12))\n",
    "months_df = months_df.withColumn('month_cos', F.cos(2 * 3.14 * F.col('month') / 12))\n",
    "\n",
    "# Adding the month since first transaction column\n",
    "months_df = months_df.withColumn('month_since_first_transaction', 23 + F.col('month'))\n",
    "\n",
    "months_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the sum(dollar_value) column to total_revenue\n",
    "months_df = months_df.withColumnRenamed('sum(dollar_value)', 'total_revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the linear model has a better performance than random forest, so we use it to predict future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add a predicted_monthly_revenue column to the dataframe\n",
    "# We will use the best linear regression model for each revenue level to predict the monthly revenue for each merchant for each month\n",
    "\n",
    "for level in revenue_levels:\n",
    "    # Load the model\n",
    "    best_model = PipelineModel.load(f'../models/revenue_linear_regression_{level}')\n",
    "    # Predict the monthly revenue for each merchant for each month\n",
    "    predictions = best_model.transform(months_df)\n",
    "\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the predictions in a parquet file\n",
    "predictions.write.mode('overwrite').parquet('../data/curated/predicted_monthly_revenue.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
